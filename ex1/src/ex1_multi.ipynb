{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Machine Learning Online Class\n",
    "  Exercise 1: Linear regression with multiple variables\n",
    "\n",
    "# Optional Exercises\n",
    "If you have successfully completed the previous notebook, congratulations! You\n",
    "now understand linear regression and should able to start using it on your\n",
    "own datasets.\n",
    "For the rest of this programming exercise, we have included the following\n",
    "optional exercises. These exercises will help you gain a deeper understanding\n",
    "of the material, and if you are able to do so, we encourage you to complete\n",
    "them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "plotlyjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Multiple Variables\n",
    "\n",
    "In this part, you will implement linear regression with multiple variables to\n",
    "predict the prices of houses. Suppose you are selling your house and you\n",
    "want to know what a good market price would be. One way to do this is to\n",
    "first collect information on recent houses sold and make a model of housing\n",
    "prices.\n",
    "The file `data/ex1data2.txt` contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the\n",
    "second column is the number of bedrooms, and the third column is the price\n",
    "of the house.\n",
    "This notebook has been set up to help you step through this exercise.\n",
    "\n",
    "\n",
    "## Feature Normalization\n",
    "We will start by loading and displaying some values from this dataset. By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly.\n",
    "Your task here is to complete the code in `feature_normalize()` to\n",
    "- Subtract the mean value of each feature from the dataset.\n",
    "- After subtracting the mean, additionally scale (divide) the feature values by their respective “standard deviations.”\n",
    "\n",
    "The standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (most data points will lie within ±2 standard deviations of the mean); this is an alternative to taking the range of values (max-min).\n",
    "\n",
    "You can use the `std()` function to compute the standard deviation. For example, inside `feature_normalize()`,\n",
    "the quantity `X[:,1]` contains all the values of `x1` (house sizes) in the training set, so `std(X[:,1])` computes the standard deviation of the house sizes.\n",
    "\n",
    "At the time that `feature_normalize()` is called, the extra column of 1’s corresponding to $x_0$ = 1 has not yet been added to `X` (see ex1 multi.m for\n",
    "details). You will do this for all the features and your code should work with datasets of all sizes (any number of features / examples). \n",
    "\n",
    "Note hat the data from the file is loaded into variable `X_raw`, which is then passed to `feature_normalize()` to compute `X_norm`. After adding the column of ones we will finally have `X`. The separation between these three variables is mainly due to ease of use in a notebok as it allows to perform each step independently of another. `X` in the text above may relate to any of those three variables.\n",
    "\n",
    "Note that each column of the matrix `X` corresponds to one feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×2 Array{Float64,2}:\n",
       " 2104.0  3.0\n",
       " 1600.0  3.0\n",
       " 2400.0  3.0\n",
       " 1416.0  2.0\n",
       " 3000.0  4.0\n",
       " 1985.0  4.0\n",
       " 1534.0  3.0\n",
       " 1427.0  3.0\n",
       " 1380.0  3.0\n",
       " 1494.0  3.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function feature_normalize(X)\n",
    "    #   feature_normalize returns a normalized version of X where\n",
    "    #   the mean value of each feature is 0 and the standard deviation\n",
    "    #   is 1. This is often a good preprocessing step to do when\n",
    "    #   working with learning algorithms.\n",
    "    \n",
    "    # Instructions: First, compute the mean 'mu' and standard deviation\n",
    "    #               'sigma' for each dimension of X.\n",
    "    #               There is a direct way to do this in Julia, check\n",
    "    #               out the help documentation for std and mean and \n",
    "    #               play around with the optional 'region' argument you\n",
    "    #               can provide to these functions in order to get the\n",
    "    #               desired result.\n",
    "    \n",
    "    #               Next you should subtract 'mu' for each feature and\n",
    "    #               then divide the features by 'sigma'.\n",
    "    \n",
    "    #               Finally return the normalized version of X, mu and \n",
    "    #               sigma.\n",
    "    #\n",
    "    #               Note that X is a matrix where each column is a \n",
    "    #               feature and each row is an example. You need \n",
    "    #               to perform the normalization separately for \n",
    "    #               each feature.\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    mu = mean(X, 1)\n",
    "    sigma = std(X, 1)\n",
    "    X_norm = (X .- mu) ./ sigma\n",
    "    # ============================================================\n",
    "    return X_norm, mu, sigma\n",
    "end\n",
    "\n",
    "## Load Data\n",
    "data = readdlm(\"../data/ex1data2.txt\", ',')\n",
    "X_raw = data[:, 1:2]\n",
    "y = data[:, 3]\n",
    "m = length(y)\n",
    "\n",
    "# Show some data points\n",
    "X_raw[1:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_norm, mu, sigma = feature_normalize(X_raw)\n",
    "\n",
    "# Add intercept term to x_norm\n",
    "X = [ones(m) X_norm];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×3 Array{Float64,2}:\n",
       " 1.0   0.13001    -0.223675\n",
       " 1.0  -0.50419    -0.223675\n",
       " 1.0   0.502476   -0.223675\n",
       " 1.0  -0.735723   -1.53777 \n",
       " 1.0   1.25748     1.09042 \n",
       " 1.0  -0.0197317   1.09042 \n",
       " 1.0  -0.58724    -0.223675\n",
       " 1.0  -0.721881   -0.223675\n",
       " 1.0  -0.781023   -0.223675\n",
       " 1.0  -0.637573   -0.223675"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 10 examples after normalization\n",
    "X[1:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Implemantation note:**\n",
    "When normalizing the features, it is important to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters\n",
    "from the model, we often want to predict the prices of houses we have not seen before. Given a new x value (living room area and number of bed-\n",
    "rooms), we must first normalize x using the mean and standard deviation\n",
    "that we had previously computed from the training set.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Gradient Descent\n",
    "Previously, you implemented gradient descent on a univariate regression\n",
    "problem. The only difference now is that there is one more feature in the matrix `X`. The hypothesis function and the batch gradient descent update rule remain unchanged.\n",
    "You should complete the code in `compute_cost_multi()` and `gradient_descent_multi()`.\n",
    "to implement the cost function and gradient descent for linear regression with multiple variables. If your code in the previous part (single variable) already\n",
    "supports multiple variables, you can use it here too.\n",
    "Make sure your code supports any number of features.\n",
    "You can use ‘size(X, 2)’ to find out how many features are present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code in `gradient_descent_multi()`. In a first step you must be able to compute the cost for multi dimensional `X`. If necessary adapt `compute_cost()` from the first part such that it can handle multi dimensional `X`.\n",
    "\n",
    "In a second step write up the code to perform gradient descent on this data in function `gradient_descent_multi!()`. The exclamation point at the end of the function name is a Julia convention for in-place functions. This means that functions ending with ! modify one or more of their input parameters. In this case you should write up code that modifies the input parameter `theta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta = [3.38658e5, 1.04128e5, -172.205]\n"
     ]
    }
   ],
   "source": [
    "function compute_cost_multi(X, y, theta)\n",
    "  # Compute cost for linear regression\n",
    "  # Computes the cost J of using theta as the\n",
    "  # parameter for linear regression to fit the data points in X and y\n",
    "\n",
    "  # Instructions: Compute the cost of a particular choice of theta\n",
    "  #               You should set J to the cost.\n",
    "  \n",
    "  # Initialize some useful values, such as the number of training examples m.\n",
    "  m = length(y)\n",
    "\n",
    "\n",
    "  # ====================== YOUR CODE HERE ======================\n",
    "  h = X * theta;\n",
    "  J = 1/(2*m) * sum((h .- y).^2)\n",
    "  # ============================================================\n",
    "  return J\n",
    "end\n",
    "\n",
    "\n",
    "function gradient_descent_multi!(theta, X, y, alpha, num_iters)\n",
    "    \n",
    "    # Initialize some useful values\n",
    "    m, n = size(X)\n",
    "    J_history = zeros(num_iters)\n",
    "    d_theta = zeros(n)\n",
    "    \n",
    "    # Hint: Perform a loop over num_iters and fill in values \n",
    "    #       J_history[iter] = compute_cost_multi(X, y, theta)\n",
    "\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    for iter in 1:num_iters\n",
    "        h = X * theta\n",
    "        for j in 1:n\n",
    "              x = X[: ,j]\n",
    "              d_theta[j] = -alpha/m * sum((h .- y) .* x)\n",
    "        end\n",
    "        theta[:] += d_theta\n",
    "        J_history[iter] = compute_cost_multi(X, y, theta)\n",
    "    end\n",
    "    # ============================================================\n",
    "\n",
    "    return J_history\n",
    "end\n",
    "\n",
    "\n",
    "# Choose some alpha value\n",
    "alpha = 0.1;\n",
    "num_iters = 50;\n",
    "\n",
    "# Init Theta and Run Gradient Descent\n",
    "theta = zeros(3);\n",
    "J_history = gradient_descent_multi!(theta, X, y, alpha, num_iters);\n",
    "@show(theta);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"63a22cc2-aca7-490b-b8c3-6b30e7b43eca\" class=\"plotly-graph-div\"></div>\n",
       "\n",
       "<script>\n",
       "    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "    window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "    require(['plotly'], function(Plotly) {\n",
       "        Plotly.newPlot('63a22cc2-aca7-490b-b8c3-6b30e7b43eca', [{\"showlegend\":true,\"mode\":\"lines\",\"xaxis\":\"x\",\"colorbar\":{\"title\":\"\"},\"line\":{\"color\":\"rgba(255, 0, 0, 1.000)\",\"width\":2,\"dash\":\"solid\",\"shape\":\"linear\"},\"y\":[0.5329407941924951,0.43427073067896155,0.35499635425210724,0.2912257172106582,0.2398666734885689,0.1984577304011999,0.16503590007117663,0.13803314822503115,0.11619502166181106,0.09851653321075932,0.08419151757715207,0.07257253419552911,0.06313905159886339,0.05547215209267677,0.04923438315939958,0.04415368104849248,0.040010523485609455,0.03662764822297399,0.033861814248433035,0.031597191975214656,0.02974005458123918,0.028214510155519575,0.026959067505448155,0.02592387051287717,0.02506846921739851,0.024360022227137602,0.023771846069166897,0.023282243831385146,0.022873558806156203,0.022531409522354523,0.022244071097057734,0.02200197468522927,0.021797302298849592,0.0216236586783953,0.02147580544564921,0.021349445619716162,0.021241048875080073,0.02114770977117746,0.021067032674934264,0.020997038301208006,0.02093608776736806,0.02088282084249811,0.020836105705200197,0.020794998035895906,0.02075870768333606,0.020726571479653257,0.020698031048959727,0.02067261467349069,0.020649922458533074,0.02062961418085968],\"type\":\"scatter\",\"name\":\"y1\",\"yaxis\":\"y\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50]}],\n",
       "               {\"showlegend\":true,\"xaxis\":{\"gridwidth\":0.5,\"tickvals\":[10.0,20.0,30.0,40.0,50.0],\"visible\":true,\"ticks\":\"inside\",\"tickmode\":\"array\",\"domain\":[0.07646908719743364,0.9934383202099737],\"linecolor\":\"rgba(0, 0, 0, 1.000)\",\"showgrid\":true,\"title\":\"iteration\",\"mirror\":false,\"tickangle\":0,\"showline\":true,\"gridcolor\":\"rgba(0, 0, 0, 0.100)\",\"titlefont\":{\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\",\"size\":15},\"tickcolor\":\"rgb(0, 0, 0)\",\"ticktext\":[\"10\",\"20\",\"30\",\"40\",\"50\"],\"zeroline\":false,\"type\":\"-\",\"tickfont\":{\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\",\"size\":11},\"zerolinecolor\":\"rgba(0, 0, 0, 1.000)\",\"anchor\":\"y\"},\"paper_bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"annotations\":[],\"height\":400,\"margin\":{\"l\":0,\"b\":20,\"r\":0,\"t\":20},\"plot_bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"yaxis\":{\"gridwidth\":0.5,\"tickvals\":[0.1,0.2,0.30000000000000004,0.4,0.5],\"visible\":true,\"ticks\":\"inside\",\"tickmode\":\"array\",\"domain\":[0.07581474190726165,0.9901574803149606],\"linecolor\":\"rgba(0, 0, 0, 1.000)\",\"showgrid\":true,\"title\":\"cost / 10^10\",\"mirror\":false,\"tickangle\":0,\"showline\":true,\"gridcolor\":\"rgba(0, 0, 0, 0.100)\",\"titlefont\":{\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\",\"size\":15},\"tickcolor\":\"rgb(0, 0, 0)\",\"ticktext\":[\"0.1\",\"0.2\",\"0.3\",\"0.4\",\"0.5\"],\"zeroline\":false,\"type\":\"-\",\"tickfont\":{\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\",\"size\":11},\"zerolinecolor\":\"rgba(0, 0, 0, 1.000)\",\"anchor\":\"x\"},\"legend\":{\"bordercolor\":\"rgba(0, 0, 0, 1.000)\",\"bgcolor\":\"rgba(255, 255, 255, 1.000)\",\"font\":{\"color\":\"rgba(0, 0, 0, 1.000)\",\"family\":\"sans-serif\",\"size\":11},\"y\":1.0,\"x\":1.0},\"width\":600}, {showLink: false});\n",
       "\n",
       "    });\n",
       " </script>\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(J_history/10e10, color=:red, width=2, yaxis=(\"cost / 10^10\"), xaxis=(\"iteration\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Optional (ungraded) exercise: Selecting learning rates\n",
    "In this part of the exercise, you will get to try out different learning rates for the dataset and find a learning rate that converges quickly. You can change the learning rate in the cell above by trying out different values for `alpha` and examine the process of the optimization process.\n",
    "\n",
    "If you picked a learning rate within a good range, your plot look similar Figure 4. If your graph looks very different, especially if your value of J(θ)\n",
    "increases or even blows up, adjust your learning rate and try again. We recommend trying values of the learning rate α on a log-scale, at multiplicative steps of about 3 times the previous value (i.e., 0.3, 0.1, 0.03, 0.01 and so on).\n",
    "You may also want to adjust the number of iterations you are running if that will help you see the overall trend in the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figures/Fig_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use this value of $\\theta$ to predict the price of a house with 1650 square feet and 3 bedrooms. You will use value later to check your implementation of the normal equations. Don’t forget to normalize your features when you make this prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): 292748.0852321537\n"
     ]
    }
   ],
   "source": [
    "# Recall that the first column of X is all-ones. Thus, it does\n",
    "# not need to be normalized.\n",
    "\n",
    "price = 0  # You should change this\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "x_new = [1650.0, 3.0]'\n",
    "x_new = (x_new .- mu) ./ sigma\n",
    "x_new = [1 x_new]\n",
    "price = dot(x_new, theta)\n",
    "# ============================================================\n",
    "\n",
    "println(\"Predicted price of a 1650 sq-ft, 3 br house (using gradient descent): $price\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Normal Equations\n",
    "In the lecture videos, you learned that the closed-form solution to linear regression is\n",
    "\n",
    "$$ \\theta = \\left( X^TX \\right)^{-1}X^Ty.$$\n",
    "\n",
    "\n",
    "Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no “loop until convergence” like in gradient descent.\n",
    "Complete the code in `normal_eqn()` to use the formula above to calculate $\\theta$. Remember that while you don’t need to scale your features, we still need to add a column of 1’s to the `X` matrix to have an intercept term $\\theta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta = [89597.9, 139.211, -8738.02]\n",
      "Predicted price of a 1650 sq-ft, 3 br house (using normal eqn): 293081.46433489607\n"
     ]
    }
   ],
   "source": [
    "function normal_eqn(X, y)\n",
    "    # Examine the inv() via the help Julia functionality\n",
    "    # and use it to compute the closed form solution\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    theta = inv(X' * X) * X' * y;\n",
    "    # ============================================================\n",
    "end\n",
    "\n",
    "# Load Data\n",
    "data = readdlm(\"../data/ex1data2.txt\", ',')\n",
    "X = data[:, 1:2]\n",
    "y = data[:, 3]\n",
    "m = length(y)\n",
    "\n",
    "# Add intercept term to X\n",
    "X = [ones(m) X];\n",
    "\n",
    "theta = normal_eqn(X, y)\n",
    "x_new = [1, 1650, 3]\n",
    "price = dot(x_new, theta)\n",
    "\n",
    "@show(theta)\n",
    "println(\"Predicted price of a 1650 sq-ft, 3 br house (using normal eqn): $price\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the predictions of house prices for the closed form solution with the gradient descent solution. Do the same thing for the two $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
